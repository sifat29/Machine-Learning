{"cells":[{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1058,"status":"ok","timestamp":1653803931743,"user":{"displayName":"fahim sifat","userId":"09905140434680475493"},"user_tz":-360},"id":"0PE498nYR2-a","outputId":"a2d1dcc3-1d67-49db-b098-0e7eb411a0c8"},"outputs":[{"name":"stdout","output_type":"stream","text":["(50000, 32, 32, 3) (50000, 10)\n","(10000, 32, 32, 3) (10000, 10)\n"]}],"source":["import tensorflow as tf\n","import numpy as np \n","import tensorflow as tf\n","from tensorflow.keras import layers, Model\n","from tensorflow.keras.applications.vgg16 import VGG16\n","\n","(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n","\n","# train set / data \n","x_train = x_train.astype('float32') / 255\n","# train set / target \n","y_train = tf.keras.utils.to_categorical(y_train , num_classes=10)\n","\n","# validation set / data \n","x_test = x_test.astype('float32') / 255\n","# validation set / target \n","y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\n","\n","print(x_train.shape, y_train.shape)\n","print(x_test.shape, y_test.shape)"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":1753,"status":"ok","timestamp":1653803937455,"user":{"displayName":"fahim sifat","userId":"09905140434680475493"},"user_tz":-360},"id":"s9HoGPi8R-n9"},"outputs":[],"source":["input = tf.keras.Input(shape=(32,32,3))\n","efnet = tf.keras.applications.EfficientNetB0(weights='imagenet',\n","                                             include_top = False, \n","                                             input_tensor = input)\n","# Now that we apply global max pooling.\n","gap = tf.keras.layers.GlobalMaxPooling2D()(efnet.output)\n","\n","# Finally, we add a classification layer.\n","output = tf.keras.layers.Dense(10, activation='softmax')(gap)\n","\n","# bind all\n","func_model = tf.keras.Model(efnet.input, output)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"63wJhTjUb4ki"},"outputs":[],"source":["def create_model():\n","\n","  model = tf.keras.models.Sequential([ \n","    tf.keras.layers.Conv2D(64,(3,3), activation = 'relu', input_shape=(256,256,3)),\n","    tf.keras.layers.MaxPooling2D(2,2),\n","\n","    tf.keras.layers.Conv2D(128,(3,3), activation = 'relu'),\n","    tf.keras.layers.MaxPooling2D(2,2),\n","\n","    tf.keras.layers.Conv2D(128,(3,3), activation = 'relu'),\n","    tf.keras.layers.MaxPooling2D(2,2),\n","\n","    tf.keras.layers.Flatten(),\n","    tf.keras.layers.Dense(512, activation = 'relu'),\n","    tf.keras.layers.Dense(1, activation='sigmoid')\n","  ])\n","\n","  \n","  model.compile(optimizer=RMSprop(lr=0.001),\n","                loss='binary_crossentropy',\n","                metrics=['accuracy']) \n","    \n","  return model"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":331170,"status":"ok","timestamp":1653804274758,"user":{"displayName":"fahim sifat","userId":"09905140434680475493"},"user_tz":-360},"id":"i442WXB8SA4N"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/15\n","391/391 - 461s - loss: 1.3316 - categorical_accuracy: 0.5324 - 461s/epoch - 1s/step\n","Epoch 2/15\n","391/391 - 452s - loss: 0.8331 - categorical_accuracy: 0.7115 - 452s/epoch - 1s/step\n","Epoch 3/15\n","391/391 - 454s - loss: 0.6586 - categorical_accuracy: 0.7710 - 454s/epoch - 1s/step\n","Epoch 4/15\n","391/391 - 454s - loss: 0.5522 - categorical_accuracy: 0.8067 - 454s/epoch - 1s/step\n","Epoch 5/15\n","391/391 - 452s - loss: 0.4701 - categorical_accuracy: 0.8374 - 452s/epoch - 1s/step\n","Epoch 6/15\n","391/391 - 452s - loss: 0.4057 - categorical_accuracy: 0.8590 - 452s/epoch - 1s/step\n","Epoch 7/15\n","391/391 - 450s - loss: 0.3503 - categorical_accuracy: 0.8783 - 450s/epoch - 1s/step\n","Epoch 8/15\n","391/391 - 453s - loss: 0.3123 - categorical_accuracy: 0.8909 - 453s/epoch - 1s/step\n","Epoch 9/15\n","391/391 - 453s - loss: 0.2717 - categorical_accuracy: 0.9054 - 453s/epoch - 1s/step\n","Epoch 10/15\n","391/391 - 451s - loss: 0.2321 - categorical_accuracy: 0.9186 - 451s/epoch - 1s/step\n","Epoch 11/15\n","391/391 - 458s - loss: 0.2107 - categorical_accuracy: 0.9264 - 458s/epoch - 1s/step\n","Epoch 12/15\n"]}],"source":["func_model.compile(\n","          loss      = tf.keras.losses.CategoricalCrossentropy(),\n","          metrics   = tf.keras.metrics.CategoricalAccuracy(),\n","          optimizer = tf.keras.optimizers.Adam())\n","# fit \n","func_model.fit(x_train, y_train, batch_size=128, epochs=15, verbose = 2)"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":428,"status":"ok","timestamp":1653803566168,"user":{"displayName":"fahim sifat","userId":"09905140434680475493"},"user_tz":-360},"id":"RkNTSik1SD_e"},"outputs":[],"source":["from tensorflow.keras.models import Model\n","import tensorflow as tf\n","import numpy as np\n","import cv2\n","\n","class GradCAM:\n","    def __init__(self, model, classIdx, layerName=None):\n","        # store the model, the class index used to measure the class\n","        # activation map, and the layer to be used when visualizing\n","        # the class activation map\n","        self.model = model\n","        self.classIdx = classIdx\n","        self.layerName = layerName\n","        # if the layer name is None, attempt to automatically find\n","        # the target output layer\n","        if self.layerName is None:\n","            self.layerName = self.find_target_layer()\n","\n","    def find_target_layer(self):\n","        # attempt to find the final convolutional layer in the network\n","        # by looping over the layers of the network in reverse order\n","        for layer in reversed(self.model.layers):\n","            # check to see if the layer has a 4D output\n","            if len(layer.output_shape) == 4:\n","                return layer.name\n","        # otherwise, we could not find a 4D layer so the GradCAM\n","        # algorithm cannot be applied\n","        raise ValueError(\"Could not find 4D layer. Cannot apply GradCAM.\")\n","\n","\n","    def compute_heatmap(self, image, eps=1e-8):\n","        # construct our gradient model by supplying (1) the inputs\n","        # to our pre-trained model, (2) the output of the (presumably)\n","        # final 4D layer in the network, and (3) the output of the\n","        # softmax activations from the model\n","        gradModel = Model(\n","            inputs=[self.model.inputs],\n","            outputs=[self.model.get_layer(self.layerName).output, self.model.output])\n","\n","        # record operations for automatic differentiation\n","        with tf.GradientTape() as tape:\n","            # cast the image tensor to a float-32 data type, pass the\n","            # image through the gradient model, and grab the loss\n","            # associated with the specific class index\n","            inputs = tf.cast(image, tf.float32)\n","            (convOutputs, predictions) = gradModel(inputs)\n","            \n","            loss = predictions[:, tf.argmax(predictions[0])]\n","    \n","        # use automatic differentiation to compute the gradients\n","        grads = tape.gradient(loss, convOutputs)\n","\n","        # compute the guided gradients\n","        castConvOutputs = tf.cast(convOutputs \u003e 0, \"float32\")\n","        castGrads = tf.cast(grads \u003e 0, \"float32\")\n","        guidedGrads = castConvOutputs * castGrads * grads\n","        # the convolution and guided gradients have a batch dimension\n","        # (which we don't need) so let's grab the volume itself and\n","        # discard the batch\n","        convOutputs = convOutputs[0]\n","        guidedGrads = guidedGrads[0]\n","\n","        # compute the average of the gradient values, and using them\n","        # as weights, compute the ponderation of the filters with\n","        # respect to the weights\n","        weights = tf.reduce_mean(guidedGrads, axis=(0, 1))\n","        cam = tf.reduce_sum(tf.multiply(weights, convOutputs), axis=-1)\n","\n","        # grab the spatial dimensions of the input image and resize\n","        # the output class activation map to match the input image\n","        # dimensions\n","        (w, h) = (image.shape[2], image.shape[1])\n","        heatmap = cv2.resize(cam.numpy(), (w, h))\n","        # normalize the heatmap such that all values lie in the range\n","        # [0, 1], scale the resulting values to the range [0, 255],\n","        # and then convert to an unsigned 8-bit integer\n","        numer = heatmap - np.min(heatmap)\n","        denom = (heatmap.max() - heatmap.min()) + eps\n","        heatmap = numer / denom\n","        heatmap = (heatmap * 255).astype(\"uint8\")\n","        # return the resulting heatmap to the calling function\n","        return heatmap\n","\n","    def overlay_heatmap(self, heatmap, image, alpha=0.5,\n","                        colormap=cv2.COLORMAP_VIRIDIS):\n","        # apply the supplied color map to the heatmap and then\n","        # overlay the heatmap on the input image\n","        heatmap = cv2.applyColorMap(heatmap, colormap)\n","        output = cv2.addWeighted(image, alpha, heatmap, 1 - alpha, 0)\n","        # return a 2-tuple of the color mapped heatmap and the output,\n","        # overlaid image\n","        return (heatmap, output)"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":4289,"status":"ok","timestamp":1653803575214,"user":{"displayName":"fahim sifat","userId":"09905140434680475493"},"user_tz":-360},"id":"XojiN-rxSG_k"},"outputs":[],"source":["image = cv2.imread('/content/image.jpg')\n","image = cv2.resize(image, (32, 32))\n","image = image.astype('float32') / 255\n","image = np.expand_dims(image, axis=0)\n","\n","preds = func_model.predict(image) \n","i = np.argmax(preds[0])"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":628,"status":"ok","timestamp":1653803575819,"user":{"displayName":"fahim sifat","userId":"09905140434680475493"},"user_tz":-360},"id":"kxMTiTeiSJS9","outputId":"814c2f62-a753-4aec-8d27-65ec7de883a1"},"outputs":[{"name":"stdout","output_type":"stream","text":["input_1\n","rescaling\n","normalization\n","stem_conv_pad\n","stem_conv\n","stem_bn\n","stem_activation\n","block1a_dwconv\n","block1a_bn\n","block1a_activation\n","block1a_se_squeeze\n","block1a_se_reshape\n","block1a_se_reduce\n","block1a_se_expand\n","block1a_se_excite\n","block1a_project_conv\n","block1a_project_bn\n","block2a_expand_conv\n","block2a_expand_bn\n","block2a_expand_activation\n","block2a_dwconv_pad\n","block2a_dwconv\n","block2a_bn\n","block2a_activation\n","block2a_se_squeeze\n","block2a_se_reshape\n","block2a_se_reduce\n","block2a_se_expand\n","block2a_se_excite\n","block2a_project_conv\n","block2a_project_bn\n","block2b_expand_conv\n","block2b_expand_bn\n","block2b_expand_activation\n","block2b_dwconv\n","block2b_bn\n","block2b_activation\n","block2b_se_squeeze\n","block2b_se_reshape\n","block2b_se_reduce\n","block2b_se_expand\n","block2b_se_excite\n","block2b_project_conv\n","block2b_project_bn\n","block2b_drop\n","block2b_add\n","block3a_expand_conv\n","block3a_expand_bn\n","block3a_expand_activation\n","block3a_dwconv_pad\n","block3a_dwconv\n","block3a_bn\n","block3a_activation\n","block3a_se_squeeze\n","block3a_se_reshape\n","block3a_se_reduce\n","block3a_se_expand\n","block3a_se_excite\n","block3a_project_conv\n","block3a_project_bn\n","block3b_expand_conv\n","block3b_expand_bn\n","block3b_expand_activation\n","block3b_dwconv\n","block3b_bn\n","block3b_activation\n","block3b_se_squeeze\n","block3b_se_reshape\n","block3b_se_reduce\n","block3b_se_expand\n","block3b_se_excite\n","block3b_project_conv\n","block3b_project_bn\n","block3b_drop\n","block3b_add\n","block4a_expand_conv\n","block4a_expand_bn\n","block4a_expand_activation\n","block4a_dwconv_pad\n","block4a_dwconv\n","block4a_bn\n","block4a_activation\n","block4a_se_squeeze\n","block4a_se_reshape\n","block4a_se_reduce\n","block4a_se_expand\n","block4a_se_excite\n","block4a_project_conv\n","block4a_project_bn\n","block4b_expand_conv\n","block4b_expand_bn\n","block4b_expand_activation\n","block4b_dwconv\n","block4b_bn\n","block4b_activation\n","block4b_se_squeeze\n","block4b_se_reshape\n","block4b_se_reduce\n","block4b_se_expand\n","block4b_se_excite\n","block4b_project_conv\n","block4b_project_bn\n","block4b_drop\n","block4b_add\n","block4c_expand_conv\n","block4c_expand_bn\n","block4c_expand_activation\n","block4c_dwconv\n","block4c_bn\n","block4c_activation\n","block4c_se_squeeze\n","block4c_se_reshape\n","block4c_se_reduce\n","block4c_se_expand\n","block4c_se_excite\n","block4c_project_conv\n","block4c_project_bn\n","block4c_drop\n","block4c_add\n","block5a_expand_conv\n","block5a_expand_bn\n","block5a_expand_activation\n","block5a_dwconv\n","block5a_bn\n","block5a_activation\n","block5a_se_squeeze\n","block5a_se_reshape\n","block5a_se_reduce\n","block5a_se_expand\n","block5a_se_excite\n","block5a_project_conv\n","block5a_project_bn\n","block5b_expand_conv\n","block5b_expand_bn\n","block5b_expand_activation\n","block5b_dwconv\n","block5b_bn\n","block5b_activation\n","block5b_se_squeeze\n","block5b_se_reshape\n","block5b_se_reduce\n","block5b_se_expand\n","block5b_se_excite\n","block5b_project_conv\n","block5b_project_bn\n","block5b_drop\n","block5b_add\n","block5c_expand_conv\n","block5c_expand_bn\n","block5c_expand_activation\n","block5c_dwconv\n","block5c_bn\n","block5c_activation\n","block5c_se_squeeze\n","block5c_se_reshape\n","block5c_se_reduce\n","block5c_se_expand\n","block5c_se_excite\n","block5c_project_conv\n","block5c_project_bn\n","block5c_drop\n","block5c_add\n","block6a_expand_conv\n","block6a_expand_bn\n","block6a_expand_activation\n","block6a_dwconv_pad\n","block6a_dwconv\n","block6a_bn\n","block6a_activation\n","block6a_se_squeeze\n","block6a_se_reshape\n","block6a_se_reduce\n","block6a_se_expand\n","block6a_se_excite\n","block6a_project_conv\n","block6a_project_bn\n","block6b_expand_conv\n","block6b_expand_bn\n","block6b_expand_activation\n","block6b_dwconv\n","block6b_bn\n","block6b_activation\n","block6b_se_squeeze\n","block6b_se_reshape\n","block6b_se_reduce\n","block6b_se_expand\n","block6b_se_excite\n","block6b_project_conv\n","block6b_project_bn\n","block6b_drop\n","block6b_add\n","block6c_expand_conv\n","block6c_expand_bn\n","block6c_expand_activation\n","block6c_dwconv\n","block6c_bn\n","block6c_activation\n","block6c_se_squeeze\n","block6c_se_reshape\n","block6c_se_reduce\n","block6c_se_expand\n","block6c_se_excite\n","block6c_project_conv\n","block6c_project_bn\n","block6c_drop\n","block6c_add\n","block6d_expand_conv\n","block6d_expand_bn\n","block6d_expand_activation\n","block6d_dwconv\n","block6d_bn\n","block6d_activation\n","block6d_se_squeeze\n","block6d_se_reshape\n","block6d_se_reduce\n","block6d_se_expand\n","block6d_se_excite\n","block6d_project_conv\n","block6d_project_bn\n","block6d_drop\n","block6d_add\n","block7a_expand_conv\n","block7a_expand_bn\n","block7a_expand_activation\n","block7a_dwconv\n","block7a_bn\n","block7a_activation\n","block7a_se_squeeze\n","block7a_se_reshape\n","block7a_se_reduce\n","block7a_se_expand\n","block7a_se_excite\n","block7a_project_conv\n","block7a_project_bn\n","top_conv\n","top_bn\n","top_activation\n","global_max_pooling2d\n","dense\n"]}],"source":["for idx in range(len(func_model.layers)):\n","  print(func_model.get_layer(index = idx).name)\n"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":415},"executionInfo":{"elapsed":450,"status":"error","timestamp":1653803718352,"user":{"displayName":"fahim sifat","userId":"09905140434680475493"},"user_tz":-360},"id":"u7GfJpedSNI9","outputId":"ef8eb6cd-27b2-46d4-d0c4-2e2cdb9c4cc6"},"outputs":[{"ename":"ValueError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-12-e11120cd43b8\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0micam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGradCAM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'block7a_project_conv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 2\u001b[0;31m \u001b[0mheatmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0micam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_heatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mheatmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/dog.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m\u003cipython-input-6-e3db610cfa0b\u003e\u001b[0m in \u001b[0;36mcompute_heatmap\u001b[0;34m(self, image, eps)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;31m# image through the gradient model, and grab the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;31m# associated with the specific class index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 45\u001b[0;31m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mconvOutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 102\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Attempt to convert a value (None) with an unsupported type (\u003cclass 'NoneType'\u003e) to a Tensor."]}],"source":["icam = GradCAM(func_model, i, 'block7a_project_conv') \n","heatmap = icam.compute_heatmap(image)\n","heatmap = cv2.resize(heatmap, (32, 32))\n","\n","image = cv2.imread('/content/dog.jpg')\n","image = cv2.resize(image, (32, 32))\n","print(heatmap.shape, image.shape)\n","\n","(heatmap, output) = icam.overlay_heatmap(heatmap, image, alpha=0.5)"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"executionInfo":{"elapsed":391,"status":"error","timestamp":1653803605578,"user":{"displayName":"fahim sifat","userId":"09905140434680475493"},"user_tz":-360},"id":"YnCa9emeSN10","outputId":"9320c6b7-dfc2-473e-f9bb-74629e809575"},"outputs":[{"ename":"NameError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-11-b2e83cab2c1a\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m()\u001b[0m\n\u001b[0;32m----\u003e 1\u001b[0;31m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"]}],"source":["fig, ax = plt.subplots(1, 3)\n","\n","ax[0].imshow(heatmap)\n","ax[1].imshow(image)\n","ax[2].imshow(output)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyN/3bYSqrK/lDdEwORILzTy","name":"gradcam with pretrained weight.ipynb","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}